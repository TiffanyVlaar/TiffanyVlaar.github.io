---
layout: page
title: Research
permalink: /research/
---

Research Interests: Understanding Foundation Models, Mathematics for Deep Learning, Sampling Methods, Numerical methods for Stochastic Differential Equations, Climate Change AI, and Physics-informed Machine Learning.

Papers: <br>
[Normalization Layers Are All That Sharpness-Aware Minimization Needs](https://arxiv.org/abs/2306.04226), *NeurIPS* 2023.

[Multirate Training of Neural Networks [blog]]({{TiffanyVlaar.github.io}}/jekyll/update/2021/06/07/Multirate)[[paper]](https://arxiv.org/abs/2106.10771)[[errata]]({{TiffanyVlaar.github.io}}/docs/Corrigendum.pdf)[[slides]]({{TiffanyVlaar.github.io}}/slides/ICML_Multirate.pdf)[[poster]]({{TiffanyVlaar.github.io}}/docs/ICML_MultiratePoster.pdf), *ICML 2022*. 

[What can linear interpolation of neural network loss landscapes tell us? [blog]]({{TiffanyVlaar.github.io}}/jekyll/update/2021/06/02/LinearInterpolation)[[paper]](https://proceedings.mlr.press/v162/vlaar22a.html)[[slides]]({{TiffanyVlaar.github.io}}/slides/ICML_LinearInterpolation.pdf)[[poster]]({{TiffanyVlaar.github.io}}/docs/Poster_LinearInterpolation_ICML.pdf), *ICML 2022*. 

[Better Training using Weight-Constrained Stochastic Dynamics [blog]]({{TiffanyVlaar.github.io}}/jekyll/update/2020/11/04/ConstraintBasedReg), *ICML 2021*. <br>
In this paper we use constrained stochastic differential equations to train deep neural networks. We provide a general framework with accompanying discretization schemes and a statistical guarantee on the convergence of the
training. This general framework enables the implementation of powerful new constraints, such as orthogonality of the weight matrix. <br>
Related workshop paper [Constraint-based Regularization of Neural Networks](https://arxiv.org/abs/2006.10114) *was accepted as spotlight presentation for NeurIPS 2020 Optimization for ML workshop and received the best student paper award.* Presentation: [YouTube link](https://youtu.be/5xhvuNPmCj4) <br>

[Partitioned Integrators for Thermodynamic Parameterization of Neural Networks]({{TiffanyVlaar.github.io}}/jekyll/update/2019/08/31/NewPaper.html)*, Foundations of Data Science 1 (4) , 457-489 (2019). Accepted as digital acceptance to NeurIPS 2019 Machine Learning and the Physical Sciences workshop.* <br>
This thermodynamic parameterization technique for neural network training allows for enhanced exploration of problems with complicated loss landscapes, which are thought to arise in molecular dynamics applications.

<!---Check out my new blogpost about the paper [here]({{TiffanyVlaar.github.io}}/jekyll/update/2019/08/31/NewPaper.html)-->



Other Research Projects:
- Deep learning framework for the unit commitment problem.
- Gerrymandering in the United Kingdom: Interpreting political geography in a statistical physics framework and using Markov Chain Monte Carlo methods for Assessing the Fairness of Political Constituencies. Related blogpost: [Gerrymandering blog]({{TiffanyVlaar.github.io}}/jekyll/update/2019/11/18/Gerrymandering.html)
- Introducing sparsity in neural networks using sampling. <br>
  Some related posts: [Generalization Error]({{TiffanyVlaar.github.io}}/jekyll/update/2019/03/27/Generalization.html), [Sparsity in Neural Networks]({{TiffanyVlaar.github.io}}/jekyll/update/2019/05/10/Sparsity.html), [Loss Landscape]({{TiffanyVlaar.github.io}}/jekyll/update/2019/07/20/LossLandscape.html).
 - Limitations of using test accuracy as the sole metric for model evaluation by examining neural network behavior at the classification boundary. Poster accepted for NeurIPS 2021 I (Still) Can’t Believe It’s Not Better (ICBINB) workshop. Preprint available on request. <br>
- Applying Bayesian inference techniques to stochastic models (in particular Langevin dynamics models) for flocking
- Stochastic Multiple Timestepping techniques for coupled systems of fast and slow variables

Selected Research Talks, Workshops, and Summerschools: 
<!--- Poster at Machine Learning and Dynamical Systems symposium, Fields Institute, 2022.-->
- Oberwolfach meeting on Constrained Dynamics, Stochastic Numerical Methods and the Modeling of Complex Systems, 2024 (TBC).
- Invited Talk, Montreal Machine Learning and Optimization Seminar, 2023.
- Invited Talk, CRM Applied Mathematics Lab Seminar, 2023.
- Invited Talk, RMT-OPT-ML Seminar, 2023.
- Invited Talk, Mathematics of Machine Learning session, CMS Summer Meeting, 2023.
- Organizer, Mathematical and Empirical Understanding of Foundation Models Workshop, ICLR 2023.
- Talk, Oxford Data Science seminar, Mathematical Institute, 2023.
- Two spotlight talks and posters at ICML, 2022.
- Poster, "Interacting particle dynamics and data science" ICMS workshop, 2022.
- Talk at "ODE and SDE methods in machine learning" symposium, NUMDIFF-16, 2021.
- Poster at Theory of Deep Learning workshop, Newton Institute, Cambridge, 2021.
- Talk and poster presenter at ICML 2021.
- Spotlight presentation at NeurIPS 2020 Optimization for ML workshop, December 2020.
- Statistical Physics & Machine Learning summer school, Les Houches, July 2020.
- Machine Learning Summer School (MLSS), Tuebingen, June 2020.
- Invited talk at Structural Inference in High Dimensional models, Mathematics of Learning conference, Bordeaux, 2020. Cancelled due to pandemic.
<!-- Poster at Foundations of Computational Mathematics, 2020. Cancelled due to pandemic.-->
- Research Collaboration Visit Gabriel Stoltz (CERMICS, Ecole des Ponts ParisTech), 2020. Cancelled due to pandemic.
- Molecular Simulation and Machine Learning conference, Temple University, Rome, January 2020.
- Sampling algorithms on manifolds workshop, Isaac Newton Institute, Cambridge, November 2019.
- Invited Poster presentation at Amazon Research Days, Edinburgh, November 2019.
- Research Collaboration Visit at Courant Institute, NYU, October 2019. Discussions with Joan Bruna, Stanislaw Jastrzebski, Mark Tuckerman, Eric Vanden-Eijnden, and Jonathan Weare (host).
- Talk at Alan Turing’s UK-Japan robotics & AI research collaboration workshop, 2019.
- Gene Golub SIAM summerschool on High-Performance Data Analytics, June 2019.
- CECAM - Computational mathematics for model reduction and predictive modelling in molecular and complex systems, May 2019.
- ICMS - Modelling Camp, May 2019.
- MIGSAA Advanced PhD Course - Diffusion Processes, Autumn 2018.
- Poster presentation at CSide Conference, University of Glasgow, November 2018.
<!--- i-like Workshop, Newcastle University, June 2018.-->
- Masterclass Prof. David Dunson (Duke University) on Scalable Bayesian Inference, 2018.
- Alan Turing Institute - Data-driven modelling of complex systems, May 2018.





