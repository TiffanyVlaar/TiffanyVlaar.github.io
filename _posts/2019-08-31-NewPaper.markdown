---
layout: post
title:  "New Paper!"
date:   2019-08-31 01:42:09 +0100
categories: jekyll update
---

We have a new preprint available on arXiv 1908.11843:
[Partitioned integrators for thermodynamic parameterization of neural networks](https://arxiv.org/abs/1908.11843)

In the paper we propose a new way to train (parameterize) neural networks for classification problems. We employ `'sampling" algorithms, which rely on discretized stochastic differential equations (SDEs), to train neural networks and show that these "thermodynamic parameterization" methods can be faster, robuster and more accurate than standard optimizers, such as SGD and ADAM,
for classification problems with complicated loss landscapes.

To understand where the term "thermodynamic parameterization" comes from we need to talk about ideas from statistical mechanics. Let us first take a bayesian perspective


<!---This paper is centered around training of neural networks for classification problems. Traditionally neural networks (NNs) are parameterized using optimization procedures such as stochastic gradient descent (SGD), RMSProp and ADAM. But in this paper we employ alternative ``sampling'' algorithms, which rely on discretized stochastic differential equations (SDEs), to train neural networks. We refer to these algorithms as thermodynamic parameterization methods, because they use an additional temperature variable to enhance the training (parameterization) process for neural networks.The idea of using an additional temperature variable originates from statistical metchanics.--> 
